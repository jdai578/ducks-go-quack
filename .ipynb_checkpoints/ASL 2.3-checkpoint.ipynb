{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, GlobalMaxPooling2D, MaxPooling2D, Conv2D, GlobalAveragePooling2D \n",
    "from numpy import array\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import keras.applications\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import os, os.path\n",
    "import shutil\n",
    "from random import shuffle\n",
    "from shutil import copyfile\n",
    "%matplotlib inline\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,     \n",
    "        rotation_range=15,\n",
    "        width_shift_range=.15,\n",
    "        height_shift_range=.15)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_path = \"/work/kaggle-link/datasets/grassknoted/asl-alphabet/asl_alphabet_train\"\n",
    "valid_path = \"/work/kaggle-link/datasets/grassknoted/asl-alphabet/asl_alphabet_valid\"\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle = True)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        valid_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(include_top=False, weights = \"imagenet\")\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) \n",
    "x = Dense(29, activation=\"softmax\")(x)\n",
    "model = Model(base_model.input, x, name=\"asl_model\")\n",
    "model.summary()\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr = 0.01,momentum=0.0, decay = 1e-4,nesterov = True)\n",
    "\n",
    "model.compile(optimizer = sgd,\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "checkpointer = ModelCheckpoint(filepath = \"/home/t-judai/log_asl201.{epoch:02d}-{val_acc:.2f}.hdf5\", monitor = \"val_acc\", verbose = 1, save_best_only = True, mode = \"auto\")\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1800,\n",
    "        epochs=30,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps=800,\n",
    "        #initial_epoch=6,\n",
    "        verbose = 1,\n",
    "        callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        \"/home/t-judai/kaggle-link/datasets/grassknoted/asl-alphabet/asl_alphabet_test\",\n",
    "        target_size = (224,224),\n",
    "        batch_size = 32,\n",
    "        class_mode = \"categorical\"\n",
    "        )\n",
    "def separateData(data_dir):\n",
    "    for filename in listdir(data_dir):\n",
    "        if isfile(join(data_dir, filename)):\n",
    "            tokens = filename.split('.')\n",
    "            if tokens[-1] == 'jpg':\n",
    "                image_path = join(data_dir, filename)\n",
    "                if not os.path.exists(join(data_dir, tokens[0])):\n",
    "                    os.makedirs(join(data_dir, tokens[0]))\n",
    "                copyfile(image_path, join(join(data_dir, tokens[0]), filename))\n",
    "                os.remove(image_path)\n",
    "#separateData(\"/home/t-judai/kaggle-link/datasets/grassknoted/asl-alphabet/asl_alphabet_test\")\n",
    "def plots(ims, figsize=(12,6), rows=4, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "        \n",
    "#test_imgs, test_labels = next(test_generator)\n",
    "#plots (test_imgs, titles = test_labels)\n",
    "\n",
    "score = model.evaluate_generator(test_generator)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time\n",
    "from PIL import Image\n",
    "import numpy\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\"\"\"\n",
    "path= \"/home/t-judai/kaggle-link/datasets/grassknoted/asl-alphabet/asl_alphabet_predict/space4.jpg\"\n",
    "#img = cv2.imread(\"/home/t-judai/kaggle-link/datasets/grassknoted/asl-alphabet/frames_from_video/a.png\")\n",
    "#pic = numpy.array(img)\n",
    "#pr = model.predict(pic.reshape((224, 224, 3)))\n",
    "img = Image.open(path)\n",
    "print (img.size)\n",
    "area = (480, 0, 1440, 1080)\n",
    "cropped_img = img.crop(area)\n",
    "print (cropped_img.size)\n",
    "cropped_img.save(path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"/home/t-judai/kaggle-link/datasets/grassknoted/asl-alphabet/asl_alphabet_predict/space4.jpg\"\n",
    "file = load_img(img,target_size = (224, 224))\n",
    "numpy_file = img_to_array(file)\n",
    "final_file = numpy.expand_dims(numpy_file, axis = 0)\n",
    "plt.imshow(np.uint8(final_file[0]))\n",
    "y_prob = model.predict(final_file/255)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "\n",
    "print (y_prob)\n",
    "print (y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/home/t-judai/saved_models/7_11_18_updatedDenseNet201.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 10\n",
    "    fig_size[1]= 12\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "classes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"del\", \"nothing\", \"space\"]\n",
    "pred = np.argmax(res, axis = 1)\n",
    "matrix = confusion_matrix(validation_generator.classes, pred)\n",
    "plot_confusion_matrix(matrix, classes, normalize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
